import streamlit as st
import pandas as pd
import os
from io import BytesIO
import requests
from requests import get
from bs4 import BeautifulSoup as bs
import glob
#--------------------------------------------------------------------------
############## CODE SCRAPING DES ORDINATEURS

def scrape_data_ordin(url):
    """Scrape les donn√©es d'une page Expat-Dakar."""
    
    res=get(url) # r√©cup√®re le code HTML de la page
    soup=bs(res.text,"html.parser") #stocker le code html dans un objet beautifulSoup ,

    contenairs=soup.find_all("div",class_="listing-card__content 1")

    data=pd.DataFrame(columns=["Details","Etat","Marque","Addresse","Prix","Lien_image"])
    
    for content in contenairs:
        try:
            details=content.find("div",class_="listing-card__header__title").text.replace("\n","").replace("  ","")
            Etat=content.find("div",class_="listing-card__header__tags").find_all("span")[0].text
            Marque=content.find("div",class_="listing-card__header__tags").find_all("span")[1].text
            Addresse=content.find("div",class_="listing-card__header__location").text.replace("\n","").replace("  ","")
            Prix=content.find("div",class_="listing-card__info-bar__price").find("span",class_="listing-card__price__value 1").text.replace("\n","").replace("F Cfa","").replace("\u202f","").replace(" ","")
            Lien_image=content.find_all("img", class_="listing-card__image__resource vh-img") #["src"]
    
            d=pd.DataFrame({"Details":[details],"Etat":[Etat],"Marque":[Marque],"Addresse":[Addresse],"Prix":[Prix],"Lien_image":[Lien_image]})
            
            data=pd.concat([data,d],ignore_index=True)
            return data
        except Exception as e:
            print(f"Erreur lors de l'extraction d'un contenu : {e}")
            return None








# Configuration de la page , layout="black"
st.set_page_config(page_title="Web Scraping App")

# Barre lat√©rale pour la navigation
menu = st.sidebar.radio(
    "Navigation",
    ["üìä Scraper des donn√©es", "üì• T√©l√©charger des donn√©es", "üìà Dashboard des donn√©es", "üìù Formulaire d'√©valuation"]
)

# 1Ô∏è‚É£ **Scraper des donn√©es en temps r√©el**
if menu == "üìä Scraper des donn√©es":
    st.title("Scraper des donn√©es")
    categorie=st.radio("Choisissez les donn√©es √† scrapper ",["Ordinateurs","T√©l√©phones","T√©l√©vision"])
    #url = st.text_input("Entrez l'URL de la page √† scraper :", "")
    
    if st.button("Lancer le scraping"):
        if categorie=="Ordinateurs":


            # Affichage
            st.dataframe(df)
        else:
            st.warning("Veuillez entrer une URL.")


def load_(dataframe, title):
    st.markdown("""
    <style>
    div.stButton {text-align:center}
    </style>""", unsafe_allow_html=True)

    #if st.button(title, key):
    st.subheader('Display data dimension')
    st.write('Data dimension: ' + str(dataframe.shape[0]) + ' rows and ' + str(dataframe.shape[1]) + ' columns.')
    st.dataframe(dataframe)


#chargement des donn√©es
df_ordinateurs=pd.read_excel('data/P1_Ordinateurs.xlsx')
df_telephones=pd.read_excel('data/P1_Telephones.xlsx')
df_television=pd.read_excel('data/P1_cinema.xlsx')

#Ajout dune liste d√©roulante dans la barre lateralle
categorie=st.sidebar.selectbox("T√©l√©charger les donn√©es d√©j√† scrapper: choisissez une cat√©gorie ",["Ordinateurs","T√©l√©phones","T√©l√©vision"])

#Affichage des donn√©es selon s√©lection
if categorie=="Ordinateurs":
    load_(df_ordinateurs,"Ordinateurs")
elif categorie=="T√©l√©phones":
    load_(df_telephones,"T√©l√©phones")
elif categorie=="T√©l√©vision":
    load_(df_television,"T√©l√©vision")

#
page=st.sidebar.selectbox("Choisissez le nombre de page √† scrapper: ",[i for i in range(1,275)])


def download_dataframe(df):
    """Permet de t√©l√©charger un DataFrame en CSV."""
    csv = df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="üì• T√©l√©charger les donn√©es",
        data=csv,
        file_name="donnees_scrapees.csv",
        mime="text/csv"
    )

url="https://www.expat-dakar.com/ordinateurs?page=1"
def scrape_data(url):
    """Scrape les donn√©es d'une page Expat-Dakar."""
    
    res=get(url) # r√©cup√®re le code HTML de la page
    soup=bs(res.text,"html.parser") #stocker le code html dans un objet beautifulSoup ,

    contenairs=soup.find_all("div",class_="listing-card__content 1")

    data=pd.DataFrame(columns=["Details","Etat","Marque","Addresse","Prix","Lien_image"])
    
    for content in contenairs:
        try:
            details=content.find("div",class_="listing-card__header__title").text.replace("\n","").replace("  ","")
            Etat=content.find("div",class_="listing-card__header__tags").find_all("span")[0].text
            Marque=content.find("div",class_="listing-card__header__tags").find_all("span")[1].text
            Addresse=content.find("div",class_="listing-card__header__location").text.replace("\n","").replace("  ","")
            Prix=content.find("div",class_="listing-card__info-bar__price").find("span",class_="listing-card__price__value 1").text.replace("\n","").replace("F Cfa","").replace("\u202f","").replace(" ","")
            Lien_image=content.find_all("img", class_="listing-card__image__resource vh-img") #["src"]
    
            d=pd.DataFrame({"Details":[details],"Etat":[Etat],"Marque":[Marque],"Addresse":[Addresse],"Prix":[Prix],"Lien_image":[Lien_image]})
            
            data=pd.concat([data,d],ignore_index=True)
            return data
        except Exception as e:
            print(f"Erreur lors de l'extraction d'un contenu : {e}")
            return None

#download_dataframe(scrape_data(url))

